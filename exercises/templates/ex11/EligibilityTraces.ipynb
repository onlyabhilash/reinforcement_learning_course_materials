{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "34e514b5cc85e765894b49805c847d1b",
     "grade": false,
     "grade_id": "cell-7357529b5cae9d71",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exercise 11) Eligibility Traces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bc6426c695dc648209171b0ef6dedbe1",
     "grade": false,
     "grade_id": "cell-3c56ee08a9f4b294",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this exercise we will explore whether we can enhance the learning behavior within the MountainCar environment when using eligibility traces. The basic idea behind eligibility traces is that - much like in n-step learning - past actions contributed to the current situation. Contrary to n-step learning, however, intuition tells us that more recent decisions had a more severe impact on the present situation than decisions that were made a long time ago. Thus, it may be helpful to integrate a forgetting factor $\\lambda$ which decreases the assumed influence of actions over time.\n",
    "\n",
    "Once again we will be looking at the MountainCar with discrete action space.\n",
    "\n",
    "![](SarsaGridworld.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe3b841b0cee883b366216220808d72d",
     "grade": false,
     "grade_id": "cell-5eb74994f421aa02",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-talk')\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from collections import deque\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8691f29907d893d6139cbf28c1e85f95",
     "grade": false,
     "grade_id": "cell-776dfe604927b326",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 0) Preprocessing Gradients\n",
    "\n",
    "For many applications we are not satisfied with directly using the `apply_gradients` function from Tensorflow. We often need to process the gradients before applying them to our ANN. E.g. in the case of Sarsa($\\lambda$) we want to compute\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{z}_\\text{new} = \\gamma \\lambda \\mathbf{z}_\\text{old} + \\nabla_\\mathbf{w}\\hat{q}(\\mathbf{x}_{k},u_{k},\\mathbf{w})\n",
    "\\end{align}\n",
    "\n",
    "before applying the gradient\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{w}_\\text{new} = \\mathbf{w}_\\text{old} + \\alpha \\delta \\mathbf{z}_\\text{new}.\n",
    "\\end{align}\n",
    "\n",
    "Thus, we need to provide for two things. Firstly, we need to calculate the gradient $\\nabla_\\mathbf{w}\\hat{q}(\\mathbf{x}_{k},u_{k},\\mathbf{w})$. Secondly, we need to process it accordingly.\n",
    "\n",
    "(Recall that in the last exercise we were applying the gradient\n",
    "\\begin{align}\n",
    "\\mathbf{w}_\\text{new} = \\mathbf{w}_\\text{old} + \\alpha \\delta \\nabla_\\mathbf{w}\\hat{q}(\\mathbf{x}_{k},u_{k},\\mathbf{w}),\n",
    "\\end{align}\n",
    "which we were able to compute easily using a quadratic loss function (mean squared error).) \n",
    "\n",
    "Note that we do not use a quadratic loss function this time! This is due to the difference in the handling of gradients in exercise 10 and 11 as presented in [this PDF](TF_Gradients.pdf).\n",
    "\n",
    "Take a look at the code snippet below and try to understand what's supposed to happen there. The code is only an excerpt, so it is not executable, but this examplary application should give a good orientation on how to tackle task (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d1f2af2851cfe9855ea369999f8a4f4",
     "grade": false,
     "grade_id": "cell-9fdc014f07ad2578",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### example: how to apply gradients manually ###\n",
    "\n",
    "# define a mean loss function\n",
    "mean = tf.keras.backend.mean\n",
    "\n",
    "# suppose that gradients are given by\n",
    "with tf.GradientTape() as tape:\n",
    "    action_values = model(norm_state)\n",
    "    loss = mean(action_values[0][action])\n",
    "gradients = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "### the following code replaces the opt.apply_gradients(zip(gradients, model.trainable_variables)) command ###\n",
    "\n",
    "# get the weights from the model, this is a list of arrays so we cannot perform calculations on it,\n",
    "# we have to carry out calculations per list element\n",
    "w = model.get_weights()\n",
    "\n",
    "# go over each element\n",
    "for i in range(len(w)):\n",
    "    w[i] += alpha * delta * gradients[i] \n",
    "    # gradients as computed per tape.gradient(...) have the same structure as model.get_weights()\n",
    "    \n",
    "# put the freshly updated weights back into the model\n",
    "model.set_weights(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "704bc8c3de9145478fd3af14f62a112c",
     "grade": false,
     "grade_id": "cell-9c72793cab79360e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1) Sarsa($\\lambda$) with Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d705e4b02e464034939b4538d116ddbf",
     "grade": false,
     "grade_id": "cell-a403525547d674ec",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Write a Sarsa($\\lambda$) algorithm to modify the learning behavior of an ANN function approximator. Test it for different values of $\\lambda$. How sensitive is the process to the choice of $\\lambda$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "25d7845cb356b9ce85dd31aa1e5b901e",
     "grade": false,
     "grade_id": "cell-73f4bb86ef59194a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_surface(model, input_dim):\n",
    "    resolution = 100\n",
    "    pos_vec = np.linspace(-1.2, 0.6, resolution)\n",
    "    vel_vec = np.linspace(-0.07, 0.07, resolution)\n",
    "\n",
    "    pos_mat, vel_mat = np.meshgrid(pos_vec, vel_vec)\n",
    "    state_tensor = np.zeros([resolution, resolution, input_dim])\n",
    "\n",
    "    for pos_idx, pos in enumerate(tqdm(pos_vec)):\n",
    "        for vel_idx, vel in enumerate(vel_vec):\n",
    "            state_tensor[vel_idx, pos_idx] = featurize(np.array([pos, vel]))\n",
    "\n",
    "    q_mat = model(state_tensor)\n",
    "    q_maxes = np.reshape(np.max(q_mat, axis=2), (resolution, resolution))\n",
    "    \n",
    "\n",
    "    # Plot\n",
    "    fig = plt.subplots()\n",
    "    ax = plt.axes(projection='3d')\n",
    "    ax.plot_surface(pos_mat, vel_mat, -q_maxes, cmap=\"viridis\")\n",
    "    ax.set_xlabel('\\n\\nposition')\n",
    "    ax.set_ylabel('\\n\\nvelocity')\n",
    "    ax.set_zlabel(r'$-V_\\mathrm{greedy}$', labelpad=12)\n",
    "    ax.view_init(50, -135)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b2fa5ac1427dd7c9d9ddfa530db761b4",
     "grade": false,
     "grade_id": "cell-348e8f33d8a7d43c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "observation_examples = np.array([env.observation_space.sample() for x in range(10000)])\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler.fit(observation_examples)\n",
    "\n",
    "featurizer = sklearn.pipeline.FeatureUnion([\n",
    "    (\"rbf0\", RBFSampler(gamma=5.0, n_components = 100)),\n",
    "    (\"rbf1\", RBFSampler(gamma=2.0, n_components = 100)),\n",
    "    (\"rbf2\", RBFSampler(gamma=1.0, n_components = 100)),\n",
    "    (\"rbf3\", RBFSampler(gamma=0.5, n_components = 100)),\n",
    "    ])\n",
    "featurizer.fit(scaler.transform(observation_examples))\n",
    "\n",
    "\n",
    "def featurize(state):\n",
    "    try:\n",
    "        scaled = scaler.transform([state])\n",
    "    except:\n",
    "        print(state)\n",
    "    featurized = featurizer.transform(scaled)\n",
    "    return featurized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "508c1d5364c29c5ec5532c43af2b930b",
     "grade": false,
     "grade_id": "cell-19a1f1e2a24895c3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "alpha = 0.0001\n",
    "gamma = 1\n",
    "_lambda = 0.1\n",
    "epsilon = 0.15\n",
    "nb_episodes = 300\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "state = env.reset()\n",
    "norm_state = featurize(state)\n",
    "input_dim = len(norm_state[0])\n",
    "\n",
    "\n",
    "# define ANN topology\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=input_dim))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(3, activation='linear'))\n",
    "\n",
    "mean = tf.keras.backend.mean\n",
    "opt = SGD(learning_rate=alpha)\n",
    "\n",
    "needed_steps_lambda = []\n",
    "\n",
    "for j in tqdm(range(nb_episodes)):\n",
    "    k = 0\n",
    "    rewards = 0\n",
    "\n",
    "    # initialize z to zero; \n",
    "    # needs to be done in a loop because get_weights and gradients are lists \n",
    "    # of arrays that preserve the structure of the ANN\n",
    "    z = model.get_weights()\n",
    "    for i in range(len(z)):\n",
    "        z[i] = z[i] * 0\n",
    "\n",
    "    state = env.reset()\n",
    "    norm_state = featurize(state)\n",
    "\n",
    "    action_values = np.squeeze(model(norm_state).numpy())\n",
    "\n",
    "    # Choose Initial Action greedy\n",
    "    if epsilon < np.random.rand(1):\n",
    "        action = np.argmax(action_values)\n",
    "    else:\n",
    "        action = random.choice(range(3))\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1f9c2ad29c99ca0306e865e1831f5fb4",
     "grade": false,
     "grade_id": "cell-d03a5f8f7a27f6e0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Greedy Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b25255e4822c62440b324c14a2eb74f0",
     "grade": false,
     "grade_id": "cell-0d4a92eac6246ae0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "state = env.reset()\n",
    "\n",
    "k = 0\n",
    "\n",
    "while True:\n",
    "    env.render()\n",
    "    k += 1\n",
    "    \n",
    "    norm_state = featurize(state)\n",
    "    action_values = np.squeeze(model(norm_state).numpy())\n",
    "    action = np.argmax(action_values)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(k)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "00544934c4aab98b9b655a1e9d882217",
     "grade": false,
     "grade_id": "cell-cfc9507d83c89469",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2) True Online Sarsa($\\lambda$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a40ed322ad1ee18d8c9e3dfbf6f21b7",
     "grade": false,
     "grade_id": "cell-e37d458b9cb6f99c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Similar to LSPI in exercise 10, also for Sarsa($\\lambda$) there exists a powerful algorithm that employs a linear approximator. This algorithm is named True Online Sarsa($\\lambda$). Let's see if the use of this algorithm shows a different behavior concerning the choice of $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9da24795df52ffe4f8325c61aef1f744",
     "grade": false,
     "grade_id": "cell-b4e146142099ac12",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "observation_examples = np.array([env.observation_space.sample() for x in range(10000)])\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler.fit(observation_examples)\n",
    "\n",
    "featurizer = sklearn.pipeline.FeatureUnion([\n",
    "    (\"rbf0\", RBFSampler(gamma=5.0, n_components = 100)),\n",
    "    (\"rbf1\", RBFSampler(gamma=2.0, n_components = 100)),\n",
    "    (\"rbf2\", RBFSampler(gamma=1.0, n_components = 100)),\n",
    "    (\"rbf3\", RBFSampler(gamma=0.5, n_components = 100)),\n",
    "    ])\n",
    "featurizer.fit(scaler.transform(observation_examples))\n",
    "\n",
    "\n",
    "def featurize(state, action):\n",
    "    action_vec = np.zeros([3, 1])\n",
    "    action_vec[action] = 1\n",
    "    \n",
    "    win = 0\n",
    "    if state[0] > 0.5:\n",
    "        win = 1\n",
    "    \n",
    "    try:\n",
    "        scaled = scaler.transform([state])\n",
    "    except:\n",
    "        print(state)\n",
    "    featurized = featurizer.transform(scaled)\n",
    "    featurized = np.reshape(featurized, (-1, 1)) # make column vector\n",
    "    \n",
    "    featurized = np.append(featurized, np.array([[1]]), axis = 0)\n",
    "    \n",
    "    featurized_vec = np.array([])\n",
    "    featurized_vec = np.expand_dims(featurized_vec, axis=-1)\n",
    "    for a in action_vec:\n",
    "        if a == 1:\n",
    "            featurized_vec = np.append(featurized_vec, featurized, axis = 0)\n",
    "        elif a == 0:\n",
    "            featurized_vec = np.append(featurized_vec, np.zeros([len(featurized), 1]), axis = 0)        \n",
    "    \n",
    "    return featurized_vec * (1 - win) # append the action to the column vector\n",
    "\n",
    "\n",
    "def policy(state, w, n, epsilon):\n",
    "    feat_states = np.zeros([len(w), n, 1])\n",
    "    q_value = np.zeros([n])\n",
    "\n",
    "    for i in range(n):    \n",
    "        feat_state = featurize(state, i)\n",
    "        feat_states[:, i] = feat_state\n",
    "        q_value[i] = np.transpose(feat_state) @ w\n",
    "            \n",
    "    if epsilon < np.random.rand(1):\n",
    "        action = np.argmax(q_value)\n",
    "    else:\n",
    "        action = random.choice(range(n))\n",
    "        \n",
    "    return feat_states[:, action], action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5da6ea675014d09d236bc0066ba7e7ff",
     "grade": false,
     "grade_id": "cell-5a059913496370ca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_surface_LSPI(w, input_dim):\n",
    "    resolution = 100\n",
    "    pos_vec = np.linspace(-1.2, 0.6, resolution)\n",
    "    vel_vec = np.linspace(-0.07, 0.07, resolution)\n",
    "\n",
    "    pos_mat, vel_mat = np.meshgrid(pos_vec, vel_vec)\n",
    "    value_tensor = np.zeros([resolution, resolution])\n",
    "\n",
    "    for pos_idx, pos in enumerate(tqdm(pos_vec)):\n",
    "        for vel_idx, vel in enumerate(vel_vec):\n",
    "            feat_state, _ =  policy(np.array([pos, vel]), w, env.action_space.n, 0)\n",
    "            value_tensor[vel_idx, pos_idx] = np.transpose(feat_state) @ w\n",
    "\n",
    "    # Plot\n",
    "    fig = plt.subplots()\n",
    "    ax = plt.axes(projection='3d')\n",
    "    ax.plot_surface(pos_mat, vel_mat, -value_tensor, cmap=\"viridis\")\n",
    "    ax.set_xlabel('\\n\\nposition')\n",
    "    ax.set_ylabel('\\n\\nvelocity')\n",
    "    ax.set_zlabel(r'$-V_\\mathrm{greedy}$', labelpad=12)\n",
    "    ax.view_init(50, -135)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f7075d25826202cd83e698365fd44865",
     "grade": false,
     "grade_id": "cell-43c5368860915b85",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "gamma = 1\n",
    "_lambda = 0.25 # we call it like that because lambda is a defined command in python\n",
    "epsilon = 0.15\n",
    "nb_episodes = 300\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "state = env.reset()\n",
    "feat_state = featurize(state, 0)\n",
    "feat_dims = len(feat_state)\n",
    "\n",
    "w = np.zeros(feat_dims)\n",
    "w = np.expand_dims(w, axis=-1)\n",
    "\n",
    "\n",
    "for j in tqdm(range(nb_episodes)):\n",
    "    k = 0\n",
    "    \n",
    "    state = env.reset()\n",
    "    feat_state, action = policy(state, w, env.action_space.n, epsilon)\n",
    "    \n",
    "    q_old = 0\n",
    "    z = np.zeros_like(feat_state)\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0e348f23a4112b1755d28b4e0d606a66",
     "grade": false,
     "grade_id": "cell-eeb61c3b42544c22",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Greedy Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "609ab9e6c9aa196832c9199de76c4ec5",
     "grade": false,
     "grade_id": "cell-2dd29436659e221f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "env.close()\n",
    "env = gym.make('MountainCar-v0')\n",
    "state = env.reset()\n",
    "\n",
    "k = 0\n",
    "\n",
    "while True:\n",
    "    env.render()\n",
    "    k += 1\n",
    "    \n",
    "    _, action = policy(state, w, env.action_space.n, 0)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(k)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "44d1ebad73ff90a19cb070bf7b05988b",
     "grade": false,
     "grade_id": "cell-2a6b7615ec8d2d8a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3) Optional: Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c677eb05b03084f1abc76fc73ff0e9a7",
     "grade": false,
     "grade_id": "cell-7433da5bc0f5d3d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Compare the algorithms from exercise 11 to the ones from exercise 10. For which value of $\\lambda$ and $\\alpha$ do they learn the fastest? Have we achieved a major breakthrough?\n",
    "\n",
    "This is covered within the corresponding tutorial video."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
